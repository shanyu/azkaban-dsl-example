// The following fields only need to be declared in hadoop block once
hadoop {
  buildPath "azkaban" // Declare the output folder for the compiled Azkaban files
  cleanPath true // Automatically clean up the output folder before each new build

  // Declare an Azkaban .properties file that sets some Azkaban properties for your jobs
  propertyFile('common') {
    set properties: [
      'failure.emails': lookupDef('notifyEmail')
      //'user.to.proxy': lookupDef('userToProxy')
    ]
  }
}

// Generic hadoop shell job that calls spark-submit
hadoopShellJob('sparkSubmitJob') {
  // must use single quote here to replace variables at runtime from properties
  uses 'spark-submit --class ${class} --master ${master} --driver-memory ${driver.memory} --executor-memory ${executor.memory} --executor-cores ${executor.cores} --num-executors ${num.executors} ${execute.jar} ${params}'
  set properties: [
    'class': 'must overwrite',
    'master': 'yarn-cluster',
    'driver.memory': '512m',
    'executor.memory': '512m',
    'executor.cores': '1',
    'num.executors': 2,
    'execute.jar': 'lib/azkaban-dsl-example.jar',
    'params': "must overwrite"
  ]
}

hadoopShellJob('uploadToDfsJob') {
  def envName = lookupDef('envName')
  if (envName.equals('local')) {
    usesCommands([
      'hadoop fs -mkdir -p ${dfs.prefix}${dfs.path}',
      'hadoop fs -copyFromLocal -f ${local.path} ${dfs.prefix}${dfs.path}'
    ])
  }
  else {
    // make sure to define properties: wasb.account, wasb.container, wasb.key for non local env
    uses 'blobxfer --storageaccountkey ${wasb.key} --remoteresource ${dfs.path} ${wasb.account} ${wasb.container} ${local.path}'
  }
  set properties: [
    'dfs.prefix': 'hdfs://mycluster/',
    'dfs.path': 'must overwrite',
    'local.path': 'must overwrite',
    'wasb.account': 'must overwrite',
    'wasb.key': 'must overwrite',
    'wasb.container': 'must overwrite'
  ]
}

